{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    h1{background-color:black; color:white; padding: 10px 10px 10px 10px}\n",
       "    h2{background-color:blue; color:white;  padding: 5px 5px 5px 5px}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "    h1{background-color:black; color:white; padding: 10px 10px 10px 10px}\n",
    "    h2{background-color:blue; color:white;  padding: 5px 5px 5px 5px}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode to run the script with.  Either 'cjc' or 'lspbs'\n",
    "mode = 'cjc' \n",
    "\n",
    "# Max number of case types to be generate the summaries for\n",
    "n_casetypes = 10\n",
    "\n",
    "# Min number of cases in for each case type in order to run summarization\n",
    "threshold = 100\n",
    "\n",
    "# Number of key phrases to be extracted for summary\n",
    "phrase_limit = 50\n",
    "\n",
    "# Word limit for significant sentences to be included in top sentences generator before (excluding similar ones)\n",
    "word_limit = 1000\n",
    "\n",
    "# Parameter to exclude sentences that are too similar from the list of top sentences.  \n",
    "# Acceptable values: between 0.0 and 1.0 inclusive\n",
    "# E.g., if sim_score_threshold = 0.9, sentences that are >= 90% similar will be excluded\n",
    "sim_score_threshold = 0.9\n",
    "\n",
    "# Boolean parameter for printing out intermediate outputs (not very useful for now)\n",
    "to_print = False\n",
    "\n",
    "# Boolean parameter for plotting the network graph\n",
    "to_plot = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytextrank in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: xlrd in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: datasketch in /opt/conda/lib/python3.6/site-packages (from pytextrank)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.6/site-packages (from pytextrank)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.6/site-packages (from pytextrank)\n",
      "Requirement already satisfied: statistics in /opt/conda/lib/python3.6/site-packages (from pytextrank)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from networkx)\n",
      "Requirement already satisfied: redis>=2.10.0 in /opt/conda/lib/python3.6/site-packages (from datasketch->pytextrank)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from datasketch->pytextrank)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: pathlib in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /opt/conda/lib/python3.6/site-packages (from spacy->pytextrank)\n",
      "Requirement already satisfied: docutils>=0.3 in /opt/conda/lib/python3.6/site-packages (from statistics->pytextrank)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy->pytextrank)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy->pytextrank)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy->pytextrank)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy->pytextrank)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy->pytextrank)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytextrank networkx xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy.en.word_sets import STOP_WORDS\n",
    "\n",
    "import pytextrank\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pylab as plt\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something to make custom pytextrank work\n",
    "__file__ = pytextrank.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harmonization_path = '../../data/anonymising_scripts/harmonize_case_types_combined.py'\n",
    "case_type_dic_filepath = '../../data/original/case_type_harmonization.csv'\n",
    "\n",
    "#CJC\n",
    "if mode.lower() == 'cjc':\n",
    "    original_file_path = '../../data/original/cjc_cases_2018_bankruptcy_20180810.xlsx'\n",
    "    col_advice = 'LEGAL_ISSUES'\n",
    "    col_synopsis = 'BACKGROUND_INFORMATION'\n",
    "    col_casetype = 'CASE_TYPE_CJC'\n",
    "#LSPBS\n",
    "elif mode.lower() == 'lspbs':\n",
    "    original_file_path = '../../data/original/lspbs_cases2016_sample.xlsx'\n",
    "    col_advice = 'ADVICE_SOUGHT'\n",
    "    col_synopsis = 'CASE_SYNOPSIS'\n",
    "    col_casetype = 'CASE_TYPE_LSPBS'\n",
    "\n",
    "%run $harmonization_path $original_file_path $mode $case_type_dic_filepath\n",
    "\n",
    "if re.search(r'xlsx', original_file_path):\n",
    "    file_path = original_file_path[:-5] + '_harmonized.csv'\n",
    "else:\n",
    "    file_path = original_file_path[:-4] + '_harmonized.csv'\n",
    "\n",
    "df = pd.read_csv(file_path).fillna('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.util.set_data_path(\"../../data/spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm/en_core_web_sm-1.2.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_STOP_WORDS = STOP_WORDS.copy()\n",
    "custom_stopwords = [\n",
    "    'adverse','party','applicant',\n",
    "    'legal','advice','general','guidance','advice','advise',\n",
    "    'like','know','seeking','lawyers','lawyer','client','help'\n",
    "    'money','amount','approached applicant','client'\n",
    "]\n",
    "\n",
    "to_keep = [\n",
    "    'who','what','when','where','why','how','whether', 'will'\n",
    "]\n",
    "to_keep = ['will']\n",
    "    \n",
    "for s in custom_stopwords:\n",
    "    NEW_STOP_WORDS.add(s)\n",
    "\n",
    "for s in to_keep:\n",
    "    NEW_STOP_WORDS.remove(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Pytextrank functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "from collections import namedtuple\n",
    "from datasketch import MinHash\n",
    "from graphviz import Digraph\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import networkx as nx\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import spacy\n",
    "import statistics\n",
    "import string\n",
    "\n",
    "DEBUG = False # True\n",
    "\n",
    "ParsedGraf = namedtuple('ParsedGraf', 'id, sha1, graf')\n",
    "WordNode = namedtuple('WordNode', 'word_id, raw, root, pos, keep, idx')\n",
    "RankedLexeme = namedtuple('RankedLexeme', 'text, rank, ids, pos, count')\n",
    "SummarySent = namedtuple('SummarySent', 'dist, idx, text')\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## filter the novel text versus quoted text in an email message\n",
    "\n",
    "PAT_FORWARD = re.compile(\"\\n\\-+ Forwarded message \\-+\\n\")\n",
    "PAT_REPLIED = re.compile(\"\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>\")\n",
    "PAT_UNSUBSC = re.compile(\"\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*\")\n",
    "\n",
    "\n",
    "def split_grafs (lines):\n",
    "    \"\"\"\n",
    "    segment the raw text into paragraphs\n",
    "    \"\"\"\n",
    "    graf = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if len(line) < 1:\n",
    "            if len(graf) > 0:\n",
    "                yield \"\\n\".join(graf)\n",
    "                graf = []\n",
    "        else:\n",
    "            graf.append(line)\n",
    "\n",
    "    if len(graf) > 0:\n",
    "        yield \"\\n\".join(graf)\n",
    "\n",
    "\n",
    "def filter_quotes (text, is_email=True):\n",
    "    \"\"\"\n",
    "    filter the quoted text out of a message\n",
    "    \"\"\"\n",
    "    global DEBUG\n",
    "    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n",
    "\n",
    "    if is_email:\n",
    "        text = filter(lambda x: x in string.printable, text)\n",
    "\n",
    "        if DEBUG:\n",
    "            print(\"text:\", text)\n",
    "\n",
    "        # strip off quoted text in a forward\n",
    "        m = PAT_FORWARD.split(text, re.M)\n",
    "\n",
    "        if m and len(m) > 1:\n",
    "            text = m[0]\n",
    "\n",
    "        # strip off quoted text in a reply\n",
    "        m = PAT_REPLIED.split(text, re.M)\n",
    "\n",
    "        if m and len(m) > 1:\n",
    "            text = m[0]\n",
    "\n",
    "        # strip off any trailing unsubscription notice\n",
    "        m = PAT_UNSUBSC.split(text, re.M)\n",
    "\n",
    "        if m:\n",
    "            text = m[0]\n",
    "\n",
    "    # replace any remaining quoted text with blank lines\n",
    "    lines = []\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line.startswith(\">\"):\n",
    "            lines.append(\"\")\n",
    "        else:\n",
    "            lines.append(line)\n",
    "\n",
    "    return list(split_grafs(lines))\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## parse and markup text paragraphs for semantic analysis\n",
    "\n",
    "PAT_PUNCT = re.compile(r'^\\W+$')\n",
    "PAT_SPACE = re.compile(r'\\_+$')\n",
    "\n",
    "POS_KEEPS = ['v', 'n', 'j']\n",
    "POS_LEMMA = ['v', 'n']\n",
    "UNIQ_WORDS = { \".\": 0 }\n",
    "\n",
    "\n",
    "def is_not_word (word):\n",
    "    return PAT_PUNCT.match(word) or PAT_SPACE.match(word)\n",
    "\n",
    "\n",
    "def get_word_id (root):\n",
    "    \"\"\"\n",
    "    lookup/assign a unique identify for each word root\n",
    "    \"\"\"\n",
    "    global UNIQ_WORDS\n",
    "\n",
    "    # in practice, this should use a microservice via some robust\n",
    "    # distributed cache, e.g., Redis, Cassandra, etc.\n",
    "    if root not in UNIQ_WORDS:\n",
    "        UNIQ_WORDS[root] = len(UNIQ_WORDS)\n",
    "\n",
    "    return UNIQ_WORDS[root]\n",
    "\n",
    "\n",
    "def fix_microsoft (foo):\n",
    "    \"\"\"\n",
    "    fix special case for `c#`, `f#`, etc.; thanks Microsoft\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    bar = []\n",
    "\n",
    "    while i < len(foo):\n",
    "        text, lemma, pos, tag = foo[i]\n",
    "\n",
    "        if (text == \"#\") and (i > 0):\n",
    "            prev_tok = bar[-1]\n",
    "\n",
    "            prev_tok[0] += \"#\"\n",
    "            prev_tok[1] += \"#\"\n",
    "\n",
    "            bar[-1] = prev_tok\n",
    "        else:\n",
    "            bar.append(foo[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return bar\n",
    "\n",
    "\n",
    "def fix_hypenation (foo):\n",
    "    \"\"\"\n",
    "    fix hyphenation in the word list for a parsed sentence\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    bar = []\n",
    "\n",
    "    while i < len(foo):\n",
    "        text, lemma, pos, tag = foo[i]\n",
    "\n",
    "        if (tag == \"HYPH\") and (i > 0) and (i < len(foo) - 1):\n",
    "            prev_tok = bar[-1]\n",
    "            next_tok = foo[i + 1]\n",
    "\n",
    "            prev_tok[0] += \"-\" + next_tok[0]\n",
    "            prev_tok[1] += \"-\" + next_tok[1]\n",
    "\n",
    "            bar[-1] = prev_tok\n",
    "            i += 2\n",
    "        else:\n",
    "            bar.append(foo[i])\n",
    "            i += 1\n",
    "\n",
    "    return bar\n",
    "\n",
    "\n",
    "def parse_graf (doc_id, graf_text, base_idx, spacy_nlp=None):\n",
    "    \"\"\"\n",
    "    CORE ALGORITHM: parse and markup sentences in the given paragraph\n",
    "    \"\"\"\n",
    "    global DEBUG\n",
    "    global POS_KEEPS, POS_LEMMA, SPACY_NLP\n",
    "\n",
    "    # set up the spaCy NLP parser\n",
    "    if not spacy_nlp:\n",
    "        if not SPACY_NLP:\n",
    "            SPACY_NLP = spacy.load(\"en_core_web_sm/en_core_web_sm-1.2.0\")\n",
    "\n",
    "        spacy_nlp = SPACY_NLP\n",
    "\n",
    "    markup = []\n",
    "    new_base_idx = base_idx\n",
    "    doc = spacy_nlp(graf_text, parse=True)\n",
    "\n",
    "    for span in doc.sents:\n",
    "        graf = []\n",
    "        digest = hashlib.sha1()\n",
    "\n",
    "        if DEBUG:\n",
    "            print(span)\n",
    "\n",
    "        # build a word list, on which to apply corrections\n",
    "        word_list = []\n",
    "\n",
    "        for tag_idx in range(span.start, span.end):\n",
    "            token = doc[tag_idx]\n",
    "\n",
    "            if DEBUG:\n",
    "                print(\"IDX\", tag_idx, token.text, token.tag_, token.pos_)\n",
    "                print(\"reg\", is_not_word(token.text))\n",
    "\n",
    "            word_list.append([token.text, token.lemma_, token.pos_, token.tag_])\n",
    "\n",
    "        # scan the parsed sentence, annotating as a list of `WordNode`\n",
    "        corrected_words = fix_microsoft(fix_hypenation(word_list))\n",
    "\n",
    "        for tok_text, tok_lemma, tok_pos, tok_tag in corrected_words:\n",
    "            word = WordNode(word_id=0, raw=tok_text, root=tok_text.lower(), pos=tok_tag, keep=0, idx=new_base_idx)\n",
    "\n",
    "            if is_not_word(tok_text) or (tok_tag == \"SYM\"):\n",
    "                # a punctuation, or other symbol\n",
    "                pos_family = '.'\n",
    "                word = word._replace(pos=pos_family)\n",
    "            else:\n",
    "                pos_family = tok_tag.lower()[0]\n",
    "\n",
    "            if pos_family in POS_LEMMA:\n",
    "                # can lemmatize this word?\n",
    "                word = word._replace(root=tok_lemma)\n",
    "\n",
    "            if pos_family in POS_KEEPS:\n",
    "                word = word._replace(word_id=get_word_id(word.root), keep=1)\n",
    "\n",
    "            digest.update(word.root.encode('utf-8'))\n",
    "\n",
    "            # schema: word_id, raw, root, pos, keep, idx\n",
    "            if DEBUG:\n",
    "                print(word)\n",
    "\n",
    "            graf.append(list(word))\n",
    "            new_base_idx += 1\n",
    "\n",
    "        markup.append(ParsedGraf(id=doc_id, sha1=digest.hexdigest(), graf=graf))\n",
    "\n",
    "    return markup, new_base_idx\n",
    "\n",
    "\n",
    "def parse_doc (json_iter):\n",
    "    \"\"\"\n",
    "    parse one document to prep for TextRank\n",
    "    \"\"\"\n",
    "    global DEBUG\n",
    "\n",
    "    for meta in json_iter:\n",
    "        base_idx = 0\n",
    "\n",
    "        for graf_text in filter_quotes(meta[\"text\"], is_email=False):\n",
    "            if DEBUG:\n",
    "                print(\"graf_text:\", graf_text)\n",
    "\n",
    "            grafs, new_base_idx = parse_graf(meta[\"id\"], graf_text, base_idx)\n",
    "            base_idx = new_base_idx\n",
    "\n",
    "            for graf in grafs:\n",
    "                yield graf\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## graph analytics\n",
    "\n",
    "def get_tiles (graf, size=3):\n",
    "    \"\"\"\n",
    "    generate word pairs for the TextRank graph\n",
    "    \"\"\"\n",
    "    keeps = list(filter(lambda w: w.word_id > 0, graf))\n",
    "    keeps_len = len(keeps)\n",
    "\n",
    "    for i in iter(range(0, keeps_len - 1)):\n",
    "        w0 = keeps[i]\n",
    "\n",
    "        for j in iter(range(i + 1, min(keeps_len, i + 1 + size))):\n",
    "            w1 = keeps[j]\n",
    "\n",
    "            if (w1.idx - w0.idx) <= size:\n",
    "                yield (w0.root, w1.root,)\n",
    "\n",
    "\n",
    "def build_graph (json_iter):\n",
    "    \"\"\"\n",
    "    construct the TextRank graph from parsed paragraphs\n",
    "    \"\"\"\n",
    "    global DEBUG, WordNode\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    for meta in json_iter:\n",
    "        if DEBUG:\n",
    "            print(meta[\"graf\"])\n",
    "\n",
    "        for pair in get_tiles(map(WordNode._make, meta[\"graf\"])):\n",
    "            if DEBUG:\n",
    "                print(pair)\n",
    "\n",
    "            for word_id in pair:\n",
    "                if not graph.has_node(word_id):\n",
    "                    graph.add_node(word_id)\n",
    "\n",
    "            try:\n",
    "                graph.adj[pair[0]][pair[1]][\"weight\"] += 1.0\n",
    "            except KeyError:\n",
    "                graph.add_edge(pair[0], pair[1], weight=1.0)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def write_dot (graph, ranks, path=\"graph.dot\"):\n",
    "    \"\"\"\n",
    "    output the graph in Dot file format\n",
    "    \"\"\"\n",
    "    dot = Digraph()\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        dot.node(node, \"%s %0.3f\" % (node, ranks[node]))\n",
    "\n",
    "    for edge in graph.edges():\n",
    "        dot.edge(edge[0], edge[1], constraint=\"false\")\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(dot.source)\n",
    "\n",
    "\n",
    "def render_ranks (graph, ranks, dot_file=\"graph.dot\"):\n",
    "    \"\"\"\n",
    "    render the TextRank graph for visual formats\n",
    "    \"\"\"\n",
    "    if dot_file:\n",
    "        write_dot(graph, ranks, path=dot_file)\n",
    "\n",
    "    ## omitted since matplotlib isn't reliable enough\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #nx.draw_networkx(graph)\n",
    "    #plt.savefig(img_file)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "def text_rank (path):\n",
    "    \"\"\"\n",
    "    run the TextRank algorithm\n",
    "    \"\"\"\n",
    "    graph = build_graph(json_iter(path))\n",
    "    ranks = nx.pagerank(graph)\n",
    "\n",
    "    return graph, ranks\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## collect key phrases\n",
    "\n",
    "SPACY_NLP = None\n",
    "STOPWORDS = NEW_STOP_WORDS\n",
    "\n",
    "STOPWORDS\n",
    "def load_stopwords (stop_file):\n",
    "    stopwords = set([])\n",
    "\n",
    "    # provide a default if needed\n",
    "    if not stop_file:\n",
    "        stop_file = \"stop.txt\"\n",
    "\n",
    "    # check whether the path is fully qualified\n",
    "    if os.path.isfile(stop_file):\n",
    "        stop_path = stop_file\n",
    "\n",
    "    # check for the file in the current working directory\n",
    "    else:\n",
    "        cwd = os.getcwd()\n",
    "        stop_path = os.path.join(cwd, stop_file)\n",
    "\n",
    "        # check for the file in the same directory as this code module\n",
    "        if not os.path.isfile(stop_path):\n",
    "            loc = os.path.realpath( os.path.join(cwd, os.path.dirname(__file__)) )\n",
    "            stop_path = os.path.join(loc, stop_file)\n",
    "\n",
    "    try:\n",
    "        with open(stop_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                stopwords.add(line.strip().lower())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def find_chunk_sub (phrase, np, i):\n",
    "    for j in iter(range(0, len(np))):\n",
    "        p = phrase[i + j]\n",
    "\n",
    "        if p.text != np[j]:\n",
    "            return None\n",
    "\n",
    "    return phrase[i:i + len(np)]\n",
    "\n",
    "\n",
    "def find_chunk (phrase, np):\n",
    "    \"\"\"\n",
    "    leverage noun phrase chunking\n",
    "    \"\"\"\n",
    "    for i in iter(range(0, len(phrase))):\n",
    "        parsed_np = find_chunk_sub(phrase, np, i)\n",
    "\n",
    "        if parsed_np:\n",
    "            return parsed_np\n",
    "\n",
    "\n",
    "def enumerate_chunks (phrase, spacy_nlp):\n",
    "    \"\"\"\n",
    "    iterate through the noun phrases\n",
    "    \"\"\"\n",
    "    if (len(phrase) > 1):\n",
    "        found = False\n",
    "        text = \" \".join([rl.text for rl in phrase])\n",
    "        doc = spacy_nlp(text.strip(), parse=True)\n",
    "\n",
    "        for np in doc.noun_chunks:\n",
    "            if np.text != text:\n",
    "                found = True\n",
    "                yield np.text, find_chunk(phrase, np.text.split(\" \"))\n",
    "\n",
    "        if not found and all([rl.pos[0] != \"v\" for rl in phrase]):\n",
    "            yield text, phrase\n",
    "\n",
    "\n",
    "def collect_keyword (sent, ranks, stopwords):\n",
    "    \"\"\"\n",
    "    iterator for collecting the single-word keyphrases\n",
    "    \"\"\"\n",
    "    for w in sent:\n",
    "        if (w.word_id > 0) and (w.root in ranks) and (w.pos[0] in \"NV\") and (w.root not in stopwords):\n",
    "            rl = RankedLexeme(text=w.raw.lower(), rank=ranks[w.root]/2.0, ids=[w.word_id], pos=w.pos.lower(), count=1)\n",
    "\n",
    "            if DEBUG:\n",
    "                print(rl)\n",
    "\n",
    "            yield rl\n",
    "\n",
    "\n",
    "def find_entity (sent, ranks, ent, i):\n",
    "    if i >= len(sent):\n",
    "        return None, None\n",
    "    else:\n",
    "        for j in iter(range(0, len(ent))):\n",
    "            w = sent[i + j]\n",
    "\n",
    "            if w.raw != ent[j]:\n",
    "                return find_entity(sent, ranks, ent, i + 1)\n",
    "\n",
    "        w_ranks = []\n",
    "        w_ids = []\n",
    "\n",
    "        for w in sent[i:i + len(ent)]:\n",
    "            w_ids.append(w.word_id)\n",
    "\n",
    "            if w.root in ranks:\n",
    "                w_ranks.append(ranks[w.root])\n",
    "            else:\n",
    "                w_ranks.append(0.0)\n",
    "\n",
    "        return w_ranks, w_ids\n",
    "\n",
    "\n",
    "def collect_entities (sent, ranks, stopwords, spacy_nlp):\n",
    "    \"\"\"\n",
    "    iterator for collecting the named-entities\n",
    "    \"\"\"\n",
    "    global DEBUG\n",
    "    sent_text = \" \".join([w.raw for w in sent])\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"sent:\", sent_text)\n",
    "\n",
    "    for ent in spacy_nlp(sent_text).ents:\n",
    "        if DEBUG:\n",
    "            print(\"NER:\", ent.label_, ent.text)\n",
    "\n",
    "        if (ent.label_ not in [\"CARDINAL\"]) and (ent.text.lower() not in stopwords):\n",
    "            w_ranks, w_ids = find_entity(sent, ranks, ent.text.split(\" \"), 0)\n",
    "\n",
    "            if w_ranks and w_ids:\n",
    "                rl = RankedLexeme(text=ent.text.lower(), rank=w_ranks, ids=w_ids, pos=\"np\", count=1)\n",
    "\n",
    "                if DEBUG:\n",
    "                    print(rl)\n",
    "\n",
    "                yield rl\n",
    "\n",
    "\n",
    "def collect_phrases (sent, ranks, spacy_nlp):\n",
    "    \"\"\"\n",
    "    iterator for collecting the noun phrases\n",
    "    \"\"\"\n",
    "    tail = 0\n",
    "    last_idx = sent[0].idx - 1\n",
    "    phrase = []\n",
    "\n",
    "    while tail < len(sent):\n",
    "        w = sent[tail]\n",
    "\n",
    "        if (w.word_id > 0) and (w.root in ranks) and ((w.idx - last_idx) == 1):\n",
    "            # keep collecting...\n",
    "            rl = RankedLexeme(text=w.raw.lower(), rank=ranks[w.root], ids=w.word_id, pos=w.pos.lower(), count=1)\n",
    "            phrase.append(rl)\n",
    "        else:\n",
    "            # just hit a phrase boundary\n",
    "            for text, p in enumerate_chunks(phrase, spacy_nlp):\n",
    "                if p:\n",
    "                    id_list = [rl.ids for rl in p]\n",
    "                    rank_list = [rl.rank for rl in p]\n",
    "                    np_rl = RankedLexeme(text=text, rank=rank_list, ids=id_list, pos=\"np\", count=1)\n",
    "\n",
    "                    if DEBUG:\n",
    "                        print(np_rl)\n",
    "\n",
    "                    yield np_rl\n",
    "\n",
    "            phrase = []\n",
    "\n",
    "        last_idx = w.idx\n",
    "        tail += 1\n",
    "\n",
    "\n",
    "def calc_rms (values):\n",
    "    \"\"\"\n",
    "    calculate a root-mean-squared metric for a list of float values\n",
    "    \"\"\"\n",
    "    #return math.sqrt(sum([x**2.0 for x in values])) / float(len(values))\n",
    "    # take the max() which works fine\n",
    "    return max(values)\n",
    "\n",
    "\n",
    "def normalize_key_phrases (path, ranks, stopwords=None, spacy_nlp=None, skip_ner=True):\n",
    "    \"\"\"\n",
    "    collect keyphrases, named entities, etc., while removing stop words\n",
    "    \"\"\"\n",
    "    global STOPWORDS, SPACY_NLP\n",
    "\n",
    "    # set up the stop words\n",
    "    if (type(stopwords) is list) or (type(stopwords) is set):\n",
    "        # explicit conversion to a set, for better performance\n",
    "        stopwords = set(stopwords)\n",
    "    else:\n",
    "        if not STOPWORDS:\n",
    "            STOPWORDS = load_stopwords(stopwords)\n",
    "\n",
    "        stopwords = STOPWORDS\n",
    "\n",
    "    # set up the spaCy NLP parser\n",
    "    if not spacy_nlp:\n",
    "        if not SPACY_NLP:\n",
    "            SPACY_NLP = spacy.load(\"en_core_web_sm/en_core_web_sm-1.2.0\")\n",
    "\n",
    "        spacy_nlp = SPACY_NLP\n",
    "\n",
    "    # collect keyphrases\n",
    "    single_lex = {}\n",
    "    phrase_lex = {}\n",
    "\n",
    "    if isinstance(path, str):\n",
    "        path = json_iter(path)\n",
    "\n",
    "    for meta in path:\n",
    "        sent = [w for w in map(WordNode._make, meta[\"graf\"])]\n",
    "\n",
    "        for rl in collect_keyword(sent, ranks, stopwords):\n",
    "            id = str(rl.ids)\n",
    "\n",
    "            if id not in single_lex:\n",
    "                single_lex[id] = rl\n",
    "            else:\n",
    "                prev_lex = single_lex[id]\n",
    "                single_lex[id] = rl._replace(count = prev_lex.count + 1)\n",
    "\n",
    "        if not skip_ner:\n",
    "            for rl in collect_entities(sent, ranks, stopwords, spacy_nlp):\n",
    "                id = str(rl.ids)\n",
    "\n",
    "                if id not in phrase_lex:\n",
    "                    phrase_lex[id] = rl\n",
    "                else:\n",
    "                    prev_lex = phrase_lex[id]\n",
    "                    phrase_lex[id] = rl._replace(count = prev_lex.count + 1)\n",
    "\n",
    "        for rl in collect_phrases(sent, ranks, spacy_nlp):\n",
    "            id = str(rl.ids)\n",
    "\n",
    "            if id not in phrase_lex:\n",
    "                phrase_lex[id] = rl\n",
    "            else:\n",
    "                prev_lex = phrase_lex[id]\n",
    "                phrase_lex[id] = rl._replace(count = prev_lex.count + 1)\n",
    "\n",
    "    # normalize ranks across single keywords and longer phrases:\n",
    "    #    * boost the noun phrases based on their length\n",
    "    #    * penalize the noun phrases for repeated words\n",
    "    rank_list = [rl.rank for rl in single_lex.values()]\n",
    "\n",
    "    if len(rank_list) < 1:\n",
    "        max_single_rank = 0\n",
    "    else:\n",
    "        max_single_rank = max(rank_list)\n",
    "\n",
    "    repeated_roots = {}\n",
    "\n",
    "    for rl in sorted(phrase_lex.values(), key=lambda rl: len(rl), reverse=True):\n",
    "        rank_list = []\n",
    "\n",
    "        for i in iter(range(0, len(rl.ids))):\n",
    "            id = rl.ids[i]\n",
    "\n",
    "            if not id in repeated_roots:\n",
    "                repeated_roots[id] = 1.0\n",
    "                rank_list.append(rl.rank[i])\n",
    "            else:\n",
    "                repeated_roots[id] += 1.0\n",
    "                rank_list.append(rl.rank[i] / repeated_roots[id])\n",
    "\n",
    "        phrase_rank = calc_rms(rank_list)\n",
    "        single_lex[str(rl.ids)] = rl._replace(rank = phrase_rank)\n",
    "\n",
    "    # scale all the ranks together, so they sum to 1.0\n",
    "    sum_ranks = sum([rl.rank for rl in single_lex.values()])\n",
    "\n",
    "    for rl in sorted(single_lex.values(), key=lambda rl: rl.rank, reverse=True):\n",
    "        if sum_ranks > 0.0:\n",
    "            rl = rl._replace(rank=rl.rank / sum_ranks)\n",
    "        elif rl.rank == 0.0:\n",
    "            rl = rl._replace(rank=0.1)\n",
    "\n",
    "        rl = rl._replace(text=re.sub(r\"\\s([\\.\\,\\-\\+\\:\\@])\\s\", r\"\\1\", rl.text))\n",
    "        yield rl\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## sentence significance\n",
    "\n",
    "def mh_digest (data):\n",
    "    \"\"\"\n",
    "    create a MinHash digest\n",
    "    \"\"\"\n",
    "    num_perm = 512\n",
    "    m = MinHash(num_perm)\n",
    "\n",
    "    for d in data:\n",
    "        m.update(d.encode('utf8'))\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def rank_kernel (path):\n",
    "    \"\"\"\n",
    "    return a list (matrix-ish) of the key phrases and their ranks\n",
    "    \"\"\"\n",
    "    kernel = []\n",
    "\n",
    "    if isinstance(path, str):\n",
    "        path = json_iter(path)\n",
    "\n",
    "    for meta in path:\n",
    "        if not isinstance(meta, RankedLexeme):\n",
    "            rl = RankedLexeme(**meta)\n",
    "        else:\n",
    "            rl = meta\n",
    "\n",
    "        m = mh_digest(map(lambda x: str(x), rl.ids))\n",
    "        kernel.append((rl, m,))\n",
    "\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def top_sentences (kernel, path):\n",
    "    \"\"\"\n",
    "    determine distance for each sentence\n",
    "    \"\"\"\n",
    "    key_sent = {}\n",
    "    i = 0\n",
    "\n",
    "    if isinstance(path, str):\n",
    "        path = json_iter(path)\n",
    "\n",
    "    for meta in path:\n",
    "        graf = meta[\"graf\"]\n",
    "        tagged_sent = [WordNode._make(x) for x in graf]\n",
    "        text = \" \".join([w.raw for w in tagged_sent])\n",
    "\n",
    "        m_sent = mh_digest([str(w.word_id) for w in tagged_sent])\n",
    "        dist = sum([m_sent.jaccard(m) * rl.rank for rl, m in kernel])\n",
    "        key_sent[text] = (dist, i)\n",
    "        i += 1\n",
    "\n",
    "    for text, (dist, i) in sorted(key_sent.items(), key=lambda x: x[1][0], reverse=True):\n",
    "        yield SummarySent(dist=dist, idx=i, text=text)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## document summarization\n",
    "\n",
    "def limit_keyphrases (path, phrase_limit=20):\n",
    "    \"\"\"\n",
    "    iterator for the most significant key phrases\n",
    "    \"\"\"\n",
    "    rank_thresh = None\n",
    "\n",
    "    if isinstance(path, str):\n",
    "        lex = []\n",
    "\n",
    "        for meta in json_iter(path):\n",
    "            rl = RankedLexeme(**meta)\n",
    "            lex.append(rl)\n",
    "    else:\n",
    "        lex = path\n",
    "\n",
    "    if len(lex) > 0:\n",
    "        rank_thresh = statistics.mean([rl.rank for rl in lex])\n",
    "    else:\n",
    "            rank_thresh = 0\n",
    "\n",
    "    used = 0\n",
    "\n",
    "    for rl in lex:\n",
    "        if rl.pos[0] != \"v\":\n",
    "            if (used > phrase_limit) or (rl.rank < rank_thresh):\n",
    "                return\n",
    "\n",
    "            used += 1\n",
    "            yield rl.text.replace(\" - \", \"-\")\n",
    "\n",
    "\n",
    "def limit_sentences (path, word_limit=100):\n",
    "    \"\"\"\n",
    "    iterator for the most significant sentences, up to a specified limit\n",
    "    \"\"\"\n",
    "    word_count = 0\n",
    "\n",
    "    if isinstance(path, str):\n",
    "        path = json_iter(path)\n",
    "\n",
    "    for meta in path:\n",
    "        if not isinstance(meta, SummarySent):\n",
    "            p = SummarySent(**meta)\n",
    "        else:\n",
    "            p = meta\n",
    "\n",
    "        sent_text = p.text.strip().split(\" \")\n",
    "        sent_len = len(sent_text)\n",
    "\n",
    "        if (word_count + sent_len) > word_limit:\n",
    "            break\n",
    "        else:\n",
    "            word_count += sent_len\n",
    "            yield sent_text, p.idx, p.dist\n",
    "\n",
    "\n",
    "def make_sentence (sent_text):\n",
    "    \"\"\"\n",
    "    construct a sentence text, with proper spacing\n",
    "    \"\"\"\n",
    "    lex = []\n",
    "    idx = 0\n",
    "\n",
    "    for word in sent_text:\n",
    "        if len(word) > 0:\n",
    "            if (idx > 0) and not (word[0] in \",.:;!?-\\\"'\"):\n",
    "                lex.append(\" \")\n",
    "\n",
    "            lex.append(word)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    return \"\".join(lex)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "## common utilities\n",
    "\n",
    "def json_iter (path):\n",
    "    \"\"\"\n",
    "    iterator for JSON-per-line in a file pattern\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def pretty_print (obj, indent=False):\n",
    "    \"\"\"\n",
    "    pretty print a JSON object\n",
    "    \"\"\"\n",
    "\n",
    "    if indent:\n",
    "        return json.dumps(obj, sort_keys=True, indent=2, separators=(',', ': '))\n",
    "    else:\n",
    "        return json.dumps(obj, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Function to extract summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_generate(df, \n",
    "                      case_type, \n",
    "                      col='advice',\n",
    "                      sim_score_threshold=0.9, \n",
    "                      phrase_limit=30, \n",
    "                      word_limit=1000, \n",
    "                      to_print=False, \n",
    "                      to_plot=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in the cleaned and harmonized case detail Pandas DataFrame, \n",
    "    performs some text processing, \n",
    "    tokenizes words using spacy-en language model and \n",
    "    prints out key phrases and summaries. \n",
    "    \n",
    "    Arguments\n",
    "    @ df                  : Cleaned and harmonized case detail Pandas DataFrame\n",
    "    @ casetype            : Case type for which the summary is to be generated\n",
    "    @ col                 : Column to be used.  Accepts 'advice', 'synopsis' and 'both' only\n",
    "    @ sim_score_threshold : Float between 0.0 and 1.0 inclusive.  Parameter to exclude sentences that are >= x in cosine similarity from the list of top sentences.  \n",
    "    @ phrase_limit        : Number of key phrases to be extracted for summary\n",
    "    @ word_limit          : Word limit for significant sentences to be included in top sentences generator before (excluding similar ones)\n",
    "    @ to_print            : Boolean parameter.  If True, prints all intermediate outputs from PyTextRank\n",
    "    @ to_plot             : Boolean parameter.  If True, plots the network graph\n",
    "    \n",
    "    Returns\n",
    "    A tuple: list of key_phrases and significant sentences (as printed out)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from column(s) to be used into a single array\n",
    "    if col.lower()=='both':\n",
    "        print('\\nColumns used: advice and synopsis')\n",
    "        advice = (df\n",
    "                  .loc[df[col_casetype] == case_type, col_advice]\n",
    "                  .append(df\n",
    "                          .loc[df[col_casetype] == case_type, col_synopsis], \n",
    "                          ignore_index=True)\n",
    "                  .values)\n",
    "    elif col.lower()=='synopsis':\n",
    "        print('\\nColumn used: synopsis')\n",
    "        advice = df.loc[df[col_casetype] == case_type, col_synopsis].values\n",
    "    else:\n",
    "        print('\\nColumn used: advice')\n",
    "        advice = df.loc[df[col_casetype] == case_type, col_advice].values\n",
    "    \n",
    "    # charset = set(list(''.join(advice)))\n",
    "    \n",
    "    # Simple text processing\n",
    "    ## Standardize some short hand notation\n",
    "    advice = [adv.replace('A/P',' Adverse Party ') for adv in advice] \n",
    "    ## Separate '#1', '#2' etc from 'Adverse Party' string to improve keyword outputs\n",
    "    advice = [adv.replace('#',' #') for adv in advice]  \n",
    "    ## Create a single long text separated by spaces after replacing double quotes as apostrophes with single quote\n",
    "    advice_all = '. '.join(advice).replace('\"', '\\'')  # \n",
    "    ## Convert to required json string format for PyTextRank\n",
    "    advice_json = '{\"id\":\"1\", \"text\":\"' + advice_all + '\"}'\n",
    "    \n",
    "    path_stage0 = 'advice.json'\n",
    "    path_stage1 = 'ol.json'\n",
    "    path_stage2 = \"o2.json\"\n",
    "    path_stage3 = \"o3.json\"\n",
    "    \n",
    "    with open(path_stage0, 'w') as f:\n",
    "        f.write(advice_json)\n",
    "    \n",
    "    with open(path_stage1, 'w') as f:\n",
    "        for graf in parse_doc(json_iter(path_stage0)):\n",
    "            f.write(\"%s\\n\" % pretty_print(graf._asdict()))\n",
    "            \n",
    "            if to_print:\n",
    "                # to view output in this notebook\n",
    "                print(pretty_print(graf))\n",
    "    \n",
    "    graph, ranks = text_rank(path_stage1)\n",
    "    render_ranks(graph, ranks)\n",
    "    \n",
    "    with open(path_stage2, 'w') as f:\n",
    "        for rl in normalize_key_phrases(path_stage1, ranks):\n",
    "            f.write(\"%s\\n\" % pretty_print(rl._asdict()))\n",
    "            \n",
    "            if to_print:\n",
    "                # to view output in this notebook\n",
    "                print(pretty_print(rl))\n",
    "    \n",
    "    if to_plot:\n",
    "        plt.figure(1, figsize = (12, 12))\n",
    "        nx.draw(graph, with_labels=True) \n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    kernel = rank_kernel(path_stage2)\n",
    "\n",
    "    with open(path_stage3, 'w') as f:\n",
    "        for s in top_sentences(kernel, path_stage1):\n",
    "            f.write(pretty_print(s._asdict()))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            if to_print:\n",
    "                # to view output in this notebook\n",
    "                print(pretty_print(s._asdict()))\n",
    "    \n",
    "    phrases = \", \".join(set([p for p in limit_keyphrases(path_stage2, phrase_limit=phrase_limit)]))\n",
    "    sent_iter = sorted(limit_sentences(path_stage3, word_limit=word_limit), key=lambda x: x[2], reverse=True)\n",
    "    s = []\n",
    "    \n",
    "    sent_text_list = []\n",
    "    counter = 1\n",
    "    for sent_text, idx, dist in sent_iter:\n",
    "        if len(sent_text_list) == 0:\n",
    "            sim_score_prev = 0.\n",
    "            sent_text_list.append(nlp(make_sentence(sent_text)))\n",
    "            s.append('sentence_' + \n",
    "                     str(counter) + \n",
    "                     ': (dist :' + \n",
    "                     str(round(dist, 5)) + \n",
    "                     ')\\n\\t - ' + \n",
    "                     make_sentence(sent_text))\n",
    "            counter += 1\n",
    "        else:\n",
    "            sim_text_curr = nlp(make_sentence(sent_text))\n",
    "            max_sim_score = max([sent_text_prev.similarity(sim_text_curr) for sent_text_prev in sent_text_list])\n",
    "            if max_sim_score < sim_score_threshold:\n",
    "                sent_text_list.append(sim_text_curr)\n",
    "                s.append('sentence_' + \n",
    "                         str(counter) + \n",
    "                         ': (dist :' + \n",
    "                        str(round(dist, 5)) + \n",
    "                         ')\\n\\t - ' + \n",
    "                         make_sentence(sent_text))\n",
    "                counter += 1\n",
    "\n",
    "    graf_text = \"\\n\".join(s)\n",
    "    \n",
    "    print(\"**excerpts:**\\n%s\\n\\n**keywords:**\\n%s\" % (graf_text, phrases,))\n",
    "    \n",
    "    return phrases, graf_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract top case types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_casestypes = (df\n",
    "                    .loc[:, [col_casetype, col_advice]]\n",
    "                    .groupby(col_casetype, as_index=False)\n",
    "                    .agg('count')\n",
    "                    .sort_values(col_advice, ascending=False)\n",
    "                    .iloc[:n_casetypes, :]\n",
    "                   )\n",
    "\n",
    "top_n_casestypes = (top_n_casestypes\n",
    "                    .loc[top_n_casestypes[col_advice] >= threshold, :]\n",
    "                   ).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Casetype: Bankruptcy / DRS (294 cases)\n",
      "\n",
      "Column used: advice\n",
      "**excerpts:**\n",
      "sentence_1: (dist :0.0544)\n",
      "\t - How to file a bankruptcy application?\n",
      "sentence_2: (dist :0.04995)\n",
      "\t - Should the applicant file for bankruptcy himself?\n",
      "sentence_3: (dist :0.04721)\n",
      "\t - Filling up of bankruptcy application form.\n",
      "sentence_4: (dist :0.04457)\n",
      "\t - Can the applicant self-declare bankruptcy instead.\n",
      "sentence_5: (dist :0.04231)\n",
      "\t - How to avoid bankruptcy from the bank?.\n",
      "sentence_6: (dist :0.0415)\n",
      "\t - Restrictions after declaring bankruptcy..\n",
      "sentence_7: (dist :0.04093)\n",
      "\t - What is the procedure for bankruptcy proceedings?\n",
      "sentence_8: (dist :0.0405)\n",
      "\t - How to file for bankruptcy and fill in Bankruptcy Forms 9, 10, 11 and 12.\n",
      "sentence_9: (dist :0.04024)\n",
      "\t - Will his employer know of his bankruptcy?\n",
      "sentence_10: (dist :0.03948)\n",
      "\t - What are the implications of bankruptcy.\n",
      "sentence_11: (dist :0.0394)\n",
      "\t - Bankruptcy proceedings; process of DRS..\n",
      "sentence_12: (dist :0.03889)\n",
      "\t - What is self-declared bankruptcy?\n",
      "sentence_13: (dist :0.03831)\n",
      "\t - Applicant wants to resist the bankruptcy application.\n",
      "sentence_14: (dist :0.03768)\n",
      "\t - ( 5 ) When can Applicant be discharged from bankruptcy?.\n",
      "sentence_15: (dist :0.03738)\n",
      "\t - Filing for bankruptcy procedural questions.\n",
      "sentence_16: (dist :0.03513)\n",
      "\t - Whether Applicant should sell the house or to declare for bankruptcy.\n",
      "sentence_17: (dist :0.03415)\n",
      "\t - Can the debt collectors bring bankruptcy actions against the applicant?\n",
      "sentence_18: (dist :0.03382)\n",
      "\t - The applicant would like a bankruptcy protection..\n",
      "sentence_19: (dist :0.03361)\n",
      "\t - 1 ) Financial Oligations/ Restrictions after bankruptcy.\n",
      "sentence_20: (dist :0.03314)\n",
      "\t - What is the estimated period of his bankruptcy?\n",
      "sentence_21: (dist :0.03245)\n",
      "\t - Applicant is asking for help in filling in forms 9- 12 for the bankruptcy application..\n",
      "sentence_22: (dist :0.0324)\n",
      "\t - The applicant came in for legal advice on his bankruptcy proposal and debt proceedings.\n",
      "sentence_23: (dist :0.02939)\n",
      "\t - He also inquired about the entire bankruptcy process..\n",
      "sentence_24: (dist :0.02877)\n",
      "\t - Issues were to do with forms and procedure in declaring bankruptcy.\n",
      "sentence_25: (dist :0.02835)\n",
      "\t - Will the OA notify the creditors of her bankruptcy or her being under a DRS?\n",
      "sentence_26: (dist :0.02604)\n",
      "\t - The applicant wants to know how to proceed to defend the bankruptcy suit..\n",
      "sentence_27: (dist :0.02575)\n",
      "\t - What is the minimum sum needed to declare bankruptcy?\n",
      "sentence_28: (dist :0.02545)\n",
      "\t - How is one's insurance policy affected by a bankruptcy order?.\n",
      "sentence_29: (dist :0.02518)\n",
      "\t - Procedure for filling out a Statement of Affairs under the Bankruptcy Act.\n",
      "sentence_30: (dist :0.02466)\n",
      "\t - 1 ) Is there a difference between self-declared bankruptcy and getting sued?\n",
      "\n",
      "**keywords:**\n",
      "court issue, pay, other outstanding debts, bankruptcy suit, banking loans, client, early discharge, judgment sums, phillipines property, creditors, bankrupt person, banrkruptcy cases, self-declared bankruptcy process, aggressive money lender, mr tiong fill, self-declared bankruptcy, bankruptcy judgment, applicant, court order, other banks, tenant file, dormant company, debt, party file, bankruptcy application form, bankruptcy proceedings, help, amount, bankrupt due, bankrupt, application, actions, hdb loan, documents, impending bankrupt person, bank, declare, mothers property, bankruptcy actions, bankruptcy order, sale, procedures, drs, application form, bankruptcy application, bankruptcy claim, proper payment, bankruptcy forms, employer know, applicant file, credit card\n"
     ]
    }
   ],
   "source": [
    "out_advice = {}\n",
    "\n",
    "for casetype, num_cases in top_n_casestypes:\n",
    "    print('\\n\\n\\nCasetype: ' + \n",
    "          casetype + \n",
    "          ' (' + \n",
    "          str(num_cases) + \n",
    "          ' cases)')\n",
    "    \n",
    "    out_advice[casetype] = textrank_generate(df, \n",
    "                                             casetype, \n",
    "                                             col='advice', \n",
    "                                             sim_score_threshold=sim_score_threshold, \n",
    "                                             phrase_limit=phrase_limit, \n",
    "                                             word_limit=word_limit, \n",
    "                                             to_print=to_print, \n",
    "                                             to_plot=to_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synposis only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Casetype: Bankruptcy / DRS (294 cases)\n",
      "\n",
      "Column used: synopsis\n",
      "**excerpts:**\n",
      "sentence_1: (dist :0.0307)\n",
      "\t - Bank filed bankruptcy against applicant.\n",
      "sentence_2: (dist :0.02885)\n",
      "\t - Banks will not file bankruptcy against him.\n",
      "sentence_3: (dist :0.02598)\n",
      "\t - Applicant facing bankruptcy.\n",
      "sentence_4: (dist :0.02493)\n",
      "\t - Applicant came from a bankruptcy hearing.\n",
      "sentence_5: (dist :0.02486)\n",
      "\t - The applicant wants to file bankruptcy.\n",
      "sentence_6: (dist :0.02477)\n",
      "\t - Bankruptcy matter.\n",
      "sentence_7: (dist :0.02432)\n",
      "\t - The applicant applied for personal insolvency ( bankruptcy ).\n",
      "sentence_8: (dist :0.02388)\n",
      "\t - The court proceeded as the applicant himself applied for bankruptcy, not the creditor..\n",
      "sentence_9: (dist :0.02308)\n",
      "\t - Applicant wants to declare bankruptcy.\n",
      "sentence_10: (dist :0.02271)\n",
      "\t - The bank is suing her for bankruptcy.\n",
      "sentence_11: (dist :0.02258)\n",
      "\t - Both of them are filing for bankruptcy due to their personal debt.\n",
      "sentence_12: (dist :0.02235)\n",
      "\t - The Applicant received a Bankruptcy Order from the Court on 28 Dec 2017.\n",
      "sentence_13: (dist :0.02231)\n",
      "\t - The applicant has declared bankruptcy.\n",
      "sentence_14: (dist :0.02156)\n",
      "\t - The applicant is a retiree and is now filing for a bankruptcy..\n",
      "sentence_15: (dist :0.02138)\n",
      "\t - The applicant came in for a potential bankruptcy advice.\n",
      "sentence_16: (dist :0.02114)\n",
      "\t - Applicant has filed for bankruptcy but his debt is less than $ 100 000.\n",
      "sentence_17: (dist :0.02105)\n",
      "\t - Applicant had his first bankruptcy court hearing today.\n",
      "sentence_18: (dist :0.021)\n",
      "\t - Now he faces bankruptcy proceedings.\n",
      "sentence_19: (dist :0.02079)\n",
      "\t - Bankruptcy: Only SD served, no application for bankruptcy yet.\n",
      "sentence_20: (dist :0.02004)\n",
      "\t - Went for court to file for bankruptcy on 23 /2.\n",
      "sentence_21: (dist :0.01985)\n",
      "\t - Client filing for bankruptcy.\n",
      "sentence_22: (dist :0.01912)\n",
      "\t - Applicant wishes to apply for bankruptcy.. Applicant wants to file for bankruptcy..\n",
      "sentence_23: (dist :0.01908)\n",
      "\t - How to file for bankruptcy and fill in Bankruptcy Forms 9, 10, 11 and 12.\n",
      "sentence_24: (dist :0.01882)\n",
      "\t - Both parties are guarantors.. Applicant has filed for a personal bankruptcy application.\n",
      "sentence_25: (dist :0.01881)\n",
      "\t - Also, Applicant to be aware of restrictions upon bankruptcy.\n",
      "sentence_26: (dist :0.01872)\n",
      "\t - Client owes a total debt of $ 120, 000 and wishes to file for bankruptcy..\n",
      "sentence_27: (dist :0.01858)\n",
      "\t - HSBC applied for statutory bankruptcy.\n",
      "sentence_28: (dist :0.01846)\n",
      "\t - This is a case of self-declared bankruptcy.\n",
      "sentence_29: (dist :0.01843)\n",
      "\t - Applicant has outstanding debts with 7 other banks as well.\n",
      "sentence_30: (dist :0.01843)\n",
      "\t - Or may issue Stat Demand ( for bankruptcy ).\n",
      "sentence_31: (dist :0.01839)\n",
      "\t - He will only file for bankruptcy as a last resort because he is currently working towards paying all his debts.\n",
      "sentence_32: (dist :0.01833)\n",
      "\t - The applicant is being sued by a vehicle leasing company for bankruptcy.\n",
      "sentence_33: (dist :0.01832)\n",
      "\t - The applicant's loan amounts to $ 80,000..\n",
      "sentence_34: (dist :0.01808)\n",
      "\t - Applicant intends to file for bankruptcy due to credit card bills.\n",
      "sentence_35: (dist :0.01788)\n",
      "\t - Applicant only carried out the bankruptcy proceedings upon recovering from the stroke.\n",
      "sentence_36: (dist :0.01776)\n",
      "\t - Applicant applied for bankruptcy.. Worked for Elite Concrete.\n",
      "sentence_37: (dist :0.01773)\n",
      "\t - Applicant owes $ 80,000-$90,000 of debt\n",
      "sentence_38: (dist :0.01744)\n",
      "\t - Applicant missed his bankruptcy hearing due an urgent meeting with potential investors for his company.\n",
      "sentence_39: (dist :0.01713)\n",
      "\t - He is involved in an on-going bankruptcy case ( 100k+ ).\n",
      "sentence_40: (dist :0.01704)\n",
      "\t - Client intends to file for self-declared bankruptcy.\n",
      "sentence_41: (dist :0.01695)\n",
      "\t - The applicant mainly faces trouble in filling up the bankruptcy forms..\n",
      "sentence_42: (dist :0.01675)\n",
      "\t - Bankruptcy status usually remains for 3- 5 years..\n",
      "sentence_43: (dist :0.01671)\n",
      "\t - - Whether Applicant is aware of effects of bankruptcy?\n",
      "sentence_44: (dist :0.01669)\n",
      "\t - Bankruptcy order given 9 March..\n",
      "\n",
      "**keywords:**\n",
      "pay, high court, personal bankruptcy, small claims tribunal, company stamp, applicant money, money, creditors, outstanding debts, hearing, loan agreement, bankruptcy, applicant, payment, other banks, credit counselling seminar, debt repayment scheme, bankruptcy charge, bankruptcy proceedings, simple loan agreement, self-declared bankrupt, current pay, company, banks, bankrupt due, constant debt, other vendors, bankrupt, electrical works, oa office, application, state courts, repayment scheme, taxi technology companies, bankruptcy case, personal liability issue, loan, bankruptcy order, ongoing business, sum, conditional s&p agreement, purchase order, letters, different banks, bankruptcy forms, bankruptcy application, file, month bond, amounts, courts singapore, debt\n"
     ]
    }
   ],
   "source": [
    "out_synopsis = {}\n",
    "\n",
    "for casetype, num_cases in top_n_casestypes:\n",
    "    print('\\n\\n\\nCasetype: ' + \n",
    "          casetype + \n",
    "          ' (' + \n",
    "          str(num_cases) + \n",
    "          ' cases)')\n",
    "    \n",
    "    out_synopsis[casetype] = textrank_generate(df, \n",
    "                                               casetype, \n",
    "                                               col='synopsis', \n",
    "                                               sim_score_threshold=sim_score_threshold, \n",
    "                                               phrase_limit=phrase_limit, \n",
    "                                               word_limit=word_limit, \n",
    "                                               to_print=to_print, \n",
    "                                               to_plot=to_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both Advice and Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Casetype: Bankruptcy / DRS (294 cases)\n",
      "\n",
      "Columns used: advice and synopsis\n",
      "**excerpts:**\n",
      "sentence_1: (dist :0.03741)\n",
      "\t - How can the applicant file for bankruptcy?.\n",
      "sentence_2: (dist :0.0363)\n",
      "\t - Bank filed bankruptcy against applicant.\n",
      "sentence_3: (dist :0.03439)\n",
      "\t - Banks will not file bankruptcy against him.\n",
      "sentence_4: (dist :0.03272)\n",
      "\t - Can the applicant self-declare bankruptcy instead.\n",
      "sentence_5: (dist :0.02985)\n",
      "\t - How to avoid bankruptcy from the bank?.\n",
      "sentence_6: (dist :0.02944)\n",
      "\t - Applicant facing bankruptcy.\n",
      "sentence_7: (dist :0.02894)\n",
      "\t - Bankruptcy matter.\n",
      "sentence_8: (dist :0.02773)\n",
      "\t - Applicant came from a bankruptcy hearing.\n",
      "sentence_9: (dist :0.02765)\n",
      "\t - The court proceeded as the applicant himself applied for bankruptcy, not the creditor..\n",
      "sentence_10: (dist :0.02752)\n",
      "\t - The applicant is a retiree and is now filing for a bankruptcy..\n",
      "sentence_11: (dist :0.02742)\n",
      "\t - ( 5 ) When can Applicant be discharged from bankruptcy?.\n",
      "sentence_12: (dist :0.02718)\n",
      "\t - Applicant's is considering filing for bankruptcy.\n",
      "sentence_13: (dist :0.02702)\n",
      "\t - Applicant wants to declare bankruptcy.\n",
      "sentence_14: (dist :0.02666)\n",
      "\t - Can the debt collectors bring bankruptcy actions against the applicant?\n",
      "sentence_15: (dist :0.02638)\n",
      "\t - The applicant has declared bankruptcy.\n",
      "sentence_16: (dist :0.0263)\n",
      "\t - Filling up of bankruptcy application form.\n",
      "sentence_17: (dist :0.0263)\n",
      "\t - How to fill in the bankruptcy application form?\n",
      "sentence_18: (dist :0.02608)\n",
      "\t - The applicant applied for personal insolvency ( bankruptcy ).\n",
      "sentence_19: (dist :0.02605)\n",
      "\t - The Applicant received a Bankruptcy Order from the Court on 28 Dec 2017.\n",
      "sentence_20: (dist :0.02592)\n",
      "\t - Applicant wants to resist the bankruptcy application.\n",
      "sentence_21: (dist :0.02561)\n",
      "\t - Applicant has filed for bankruptcy but his debt is less than $ 100 000.\n",
      "sentence_22: (dist :0.02556)\n",
      "\t - Went for court to file for bankruptcy on 23 /2.\n",
      "sentence_23: (dist :0.02547)\n",
      "\t - How to file for bankruptcy and fill in Bankruptcy Forms 9, 10, 11 and 12.\n",
      "sentence_24: (dist :0.02526)\n",
      "\t - The bank is suing her for bankruptcy.\n",
      "sentence_25: (dist :0.02526)\n",
      "\t - The applicant came in for legal advice on his bankruptcy proposal and debt proceedings.\n",
      "sentence_26: (dist :0.02508)\n",
      "\t - Client intends to file for bankruptcy.\n",
      "sentence_27: (dist :0.02502)\n",
      "\t - Now he faces bankruptcy proceedings.\n",
      "sentence_28: (dist :0.02492)\n",
      "\t - Whether Applicant should sell the house or to declare for bankruptcy.\n",
      "sentence_29: (dist :0.02483)\n",
      "\t - The applicant would like a bankruptcy protection..\n",
      "sentence_30: (dist :0.02482)\n",
      "\t - Restrictions after declaring bankruptcy..\n",
      "sentence_31: (dist :0.02461)\n",
      "\t - What is the procedure for bankruptcy proceedings?\n",
      "sentence_32: (dist :0.02446)\n",
      "\t - Bankruptcy: Only SD served, no application for bankruptcy yet.\n",
      "sentence_33: (dist :0.02442)\n",
      "\t - Will his employer know of his bankruptcy?\n",
      "sentence_34: (dist :0.02435)\n",
      "\t - Applicant wishes to apply for bankruptcy.. Applicant wants to file for bankruptcy..\n",
      "sentence_35: (dist :0.02425)\n",
      "\t - Filing for bankruptcy procedural questions.\n",
      "sentence_36: (dist :0.02424)\n",
      "\t - What is self-declared bankruptcy?\n",
      "sentence_37: (dist :0.02396)\n",
      "\t - Bankruptcy proceedings; process of DRS..\n",
      "sentence_38: (dist :0.02389)\n",
      "\t - What are the implications of bankruptcy.\n",
      "sentence_39: (dist :0.02359)\n",
      "\t - Applicant had his first bankruptcy court hearing today.\n",
      "sentence_40: (dist :0.02286)\n",
      "\t - The applicant has not attended any bankruptcy hearings..\n",
      "sentence_41: (dist :0.02267)\n",
      "\t - Applicant intends to file for bankruptcy due to credit card bills.\n",
      "sentence_42: (dist :0.02249)\n",
      "\t - Client owes a total debt of $ 120, 000 and wishes to file for bankruptcy..\n",
      "sentence_43: (dist :0.02238)\n",
      "\t - Or may issue Stat Demand ( for bankruptcy ).\n",
      "sentence_44: (dist :0.02221)\n",
      "\t - Both parties are guarantors.. Applicant has filed for a personal bankruptcy application.\n",
      "sentence_45: (dist :0.02218)\n",
      "\t - HSBC applied for statutory bankruptcy.\n",
      "sentence_46: (dist :0.02193)\n",
      "\t - Also, Applicant to be aware of restrictions upon bankruptcy.\n",
      "sentence_47: (dist :0.02169)\n",
      "\t - He will only file for bankruptcy as a last resort because he is currently working towards paying all his debts.\n",
      "sentence_48: (dist :0.02113)\n",
      "\t - This is a case of self-declared bankruptcy.\n",
      "\n",
      "**keywords:**\n",
      "court issue, pay, other outstanding debts, banking loans, bankruptcy suit, credit card companies, judgment sums, first bankruptcy court hearing, applicant money, creditors, bankrupt person, banrkruptcy cases, applicant file, aggressive money lender, self-declared bankruptcy process, bankruptcy application forms, mr tiong fill, bankruptcy judgment, applicant, court order, other banks, business reputation, dormant company, tenant file, party file, debt repayment scheme, bankruptcy proceedings, simple loan agreement, company, current pay, banks, bankrupt due, housing loans, personal hdb, bankrupt, debt restructuring scheme, bank accounts, bankruptcy action, debt collectors, hdb loan, impending bankrupt person, declare, mothers property, bankruptcy order, letters, proper payment, bankruptcy application, bankruptcy claim, amounts, debt, credit card\n"
     ]
    }
   ],
   "source": [
    "out_both = {}\n",
    "\n",
    "for casetype, num_cases in top_n_casestypes:\n",
    "    print('\\n\\n\\nCasetype: ' + \n",
    "          casetype + \n",
    "          ' (' + \n",
    "          str(num_cases) + \n",
    "          ' cases)')\n",
    "    \n",
    "    out_both[casetype] = textrank_generate(df, \n",
    "                                           casetype, \n",
    "                                           col='both', \n",
    "                                           sim_score_threshold=sim_score_threshold, \n",
    "                                           phrase_limit=phrase_limit, \n",
    "                                           word_limit=word_limit, \n",
    "                                           to_print=to_print, \n",
    "                                           to_plot=to_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
